{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "from langdetect import DetectorFactory\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    DetectorFactory.seed = 0\n",
    "    punctuation = \"Î¾\"\n",
    "    tokens = [token.strip(punctuation) for token in nltk.word_tokenize(text)]\n",
    "    text = \" \".join(tokens)\n",
    "#     text = re.sub('\\W+',\" \", text)\n",
    "    if len(text) >= 200:\n",
    "#         langs = detect_langs(text)\n",
    "#         main_lang = detect(text)\n",
    "        return text\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import string\n",
    "import nltk\n",
    "def extract_candidate_words(\n",
    "        text,\n",
    "        good_tags=set(['JJ', 'JJR', 'JJS', 'NN', 'NNP', 'NNS', 'NNPS', 'VB', 'VBD',\n",
    "                      'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "        ):\n",
    "    '''Exclude candidates that are stop words or entirely punctuation.\n",
    "    '''\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize and POS-tag words\n",
    "    tagged_words = itertools.chain.from_iterable(\n",
    "        nltk.pos_tag_sents(\n",
    "            nltk.word_tokenize(sent) \n",
    "            for sent in nltk.sent_tokenize(text)\n",
    "        )\n",
    "    )\n",
    "    # filter on certain POS tags and lowercase all words\n",
    "    candidates = [word.lower() for word, tag in tagged_words\n",
    "                  if tag in good_tags and word.lower() not in stop_words\n",
    "                  and not all(char in punct for char in word)]\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trie_search\n",
    "def create_trie_from_patterns(file_path):\n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    patterns = ''.join(file.readlines())\n",
    "    patterns = patterns.splitlines()\n",
    "    file.close()\n",
    "    return trie_search.TrieSearch(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_words(words, tag, candidates):\n",
    "    res = \"\"\n",
    "    chunk = \"B-\"\n",
    "    for word in nltk.word_tokenize(words):\n",
    "        if word.lower() not in candidates:\n",
    "            tag_curr = 'O'\n",
    "            chunk = \"B-\"\n",
    "        elif tag != \"O\":\n",
    "            tag_curr = chunk + tag\n",
    "            chunk = \"I-\"\n",
    "        else:\n",
    "            tag_curr = tag\n",
    "        res += word + \" \" + tag_curr + \"\\n\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def annotate_text(text):\n",
    "    text = \"\\n\" + text + \"\\n\"\n",
    "    trie = create_trie_from_patterns(\"data/skills_lower.txt\")\n",
    "    annotated_text = \"\"\n",
    "    candidates = extract_candidate_words(text)\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        sentence = \" \".join(tokens)\n",
    "        end = 0\n",
    "        for pattern, start in sorted(trie.search_longest_patterns(sentence.lower()),\n",
    "                                     key=lambda x: x[1]):\n",
    "            if start != end:\n",
    "                words = sentence[end:start].strip()\n",
    "                res = annotate_words(words, \"O\", candidates)\n",
    "                annotated_text += res\n",
    "            end = start + len(pattern)\n",
    "            \n",
    "            res = annotate_words(sentence[start:end], \"SKL\", candidates)\n",
    "            annotated_text += res\n",
    "            \n",
    "        words = sentence[end:].strip()\n",
    "        res = annotate_words(words, \"O\", candidates)\n",
    "        annotated_text += res + \"\\n\"\n",
    "    return annotated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data/sample_CV.txt\", 'r', encoding=\"utf-8\")\n",
    "text = \"\".join(file.readlines())\n",
    "file.close()\n",
    "words_tag = annotate_text(text)\n",
    "print(words_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('data/wcn_cv.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "start_time = time.time()\n",
    "texts = list(df['cv_txt'])\n",
    "pool = Pool()\n",
    "annotated_texts = pool.map(annotate_text, texts[:2000])\n",
    "pool.close()\n",
    "pool.join()\n",
    "    \n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "file = open(\"sequence_tagging/data/train.txt\", \"w\", encoding=\"utf-8\")\n",
    "begin = 0\n",
    "end = math.ceil(len(annotated_texts) * 0.6)\n",
    "[file.writelines(text) for text in annotated_texts[begin:end]]\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"sequence_tagging/data/val.txt\", \"w\", encoding=\"utf-8\")\n",
    "begin = math.ceil(len(annotated_texts) * 0.6)\n",
    "end = math.ceil(len(annotated_texts) * 0.8)\n",
    "[file.writelines(text) for text in annotated_texts[begin:end]]\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"sequence_tagging/data/test.txt\", \"w\", encoding=\"utf-8\")\n",
    "begin = math.ceil(len(annotated_texts) * 0.8)\n",
    "end = math.ceil(len(annotated_texts))\n",
    "[file.writelines(text) for text in annotated_texts[begin:end]]\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l sequence_tagging/data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head sequence_tagging/data/train.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
